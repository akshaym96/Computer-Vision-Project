{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are photo_id, business_id\n",
      "Processed 234843 lines.\n",
      "234842\n",
      "Column names are business_id, labels\n",
      "Processed 2001 lines.\n",
      "2000\n",
      "Histogram distribution for the labels in the training_set\n",
      "[9163, 23679, 25603, 19453, 16829, 29454, 30772, 14825, 20300]\n"
     ]
    }
   ],
   "source": [
    "#CSV file reading\n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for photo_id ---> buisness_id\n",
    "\"\"\"\n",
    "photo_to_bus_dict = {}\n",
    "with open('train_photo_to_biz_ids.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            photo_to_bus_dict[row[0]] = row[1]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "print(len(photo_to_bus_dict))\n",
    "    \n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for business_id ---> labels\n",
    "\"\"\"\n",
    "bus_to_labels_dict = {}     \n",
    "with open('train.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            bus_to_labels_dict[row[0]] = [row[1]]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')  \n",
    "print(len(bus_to_labels_dict))\n",
    "\n",
    "\"\"\"\n",
    "Checking the distribution of labels\n",
    "in the training set\n",
    "\"\"\"\n",
    "hist_label = [0]*9\n",
    "for filename in os.listdir('training_set'):\n",
    "    photo_id = filename.split('.')\n",
    "    lst_labels = bus_to_labels_dict[photo_to_bus_dict[photo_id[0]]]\n",
    "#     print(lst_labels)\n",
    "    labels = lst_labels[0].split(' ')\n",
    "#     print(labels)\n",
    "    if len(labels)!=0:\n",
    "        for i in labels:\n",
    "            if i is not '':\n",
    "                hist_label[int(i)]+=1\n",
    "\n",
    "print(\"Histogram distribution for the labels in the training_set\")\n",
    "print(hist_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10195.jpg\n",
      "20062.jpg\n",
      "29986.jpg\n",
      "40137.jpg\n",
      "50092.jpg\n",
      "59987.jpg\n",
      "70098.jpg\n",
      "79934.jpg\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the training data\n",
    "Returns images and respective photo ids\n",
    "\"\"\"\n",
    "def get_train_data():\n",
    "    data = []\n",
    "    photo_ids = []\n",
    "    count=0\n",
    "    for filename in sorted(os.listdir('training_set/'),key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        if count%5000==4999:\n",
    "            print(filename)\n",
    "        photo_ids.append(filename.split('.')[0])\n",
    "        data.append(cv2.imread('training_set/'+filename,0))\n",
    "        count+=1\n",
    "    return data,photo_ids\n",
    "    \n",
    "train_data, train_photo_ids = get_train_data()\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10195.jpg\n",
      "20062.jpg\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the testing data\n",
    "Returns images and respective photo ids\n",
    "\"\"\"\n",
    "def get_test_data():\n",
    "    data = []\n",
    "    photo_ids = []\n",
    "    count=0\n",
    "    for filename in sorted(os.listdir('testing_set/'),key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        if count%5000==4999:\n",
    "            print(filename)\n",
    "        photo_ids.append(filename.split('.')[0])\n",
    "        data.append(cv2.imread('testing_set/'+filename,0))\n",
    "        count+=1\n",
    "    return data,photo_ids\n",
    "    \n",
    "test_data, test_photo_ids = get_test_data()\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "def id_to_label(photo_ids):\n",
    "    photo_labels = np.empty(shape=[0,9])\n",
    "    for id in photo_ids:\n",
    "        lst_labels = bus_to_labels_dict[photo_to_bus_dict[id]]\n",
    "        ls = lst_labels[0].split(' ')\n",
    "        labels = [0]*9\n",
    "        for label in ls:\n",
    "            if label !='':\n",
    "                labels[int(label)] = 1\n",
    "        labels = np.array(labels).reshape(1,-1)\n",
    "        photo_labels = np.append(photo_labels,labels,axis=0)\n",
    "    return photo_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1.  0.  1.  1.  1.  1.  0.]\n",
      "['1 2 4 5 6 7']\n",
      "(40000, 9)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "\"\"\"\n",
    "Contains no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "train_photo_labels = id_to_label(train_photo_ids)\n",
    "print(train_photo_labels[0])\n",
    "print(bus_to_labels_dict[photo_to_bus_dict[train_photo_ids[0]]])\n",
    "print(train_photo_labels.shape)\n",
    "print(train_photo_labels[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1.  0.  1.  1.  1.  1.  0.]\n",
      "['1 2 4 5 6 7']\n",
      "(10000, 9)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Testing set\n",
    "\"\"\"\n",
    "Contains no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "test_photo_labels = id_to_label(test_photo_ids)\n",
    "print(test_photo_labels[0])\n",
    "print(bus_to_labels_dict[photo_to_bus_dict[test_photo_ids[0]]])\n",
    "print(test_photo_labels.shape)\n",
    "print(test_photo_labels[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of overall training images descriptors:-\n",
      "(3132260, 128)\n",
      "Extracting training images descriptors took 94.181s.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting descriptors for training data\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "with open(\"descriptors_train.sav\",'rb') as fp:\n",
    "    train_individual_des, train_des=pickle.load(fp)\n",
    "\n",
    "print(\"Shape of overall training images descriptors:-\")\n",
    "print(train_des.shape)\n",
    "print(\"Extracting training images descriptors took %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of overall test images descriptors:-\n",
      "(782829, 128)\n",
      "Extracting test images descriptors took 86.721s.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting descriptors for testing data\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "with open(\"descriptors_test.sav\",'rb') as fp:\n",
    "    test_individual_des, test_des =pickle.load(fp)\n",
    "\n",
    "print(\"Shape of overall test images descriptors:-\")\n",
    "print(test_des.shape)\n",
    "print(\"Extracting test images descriptors took %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the K-Means clustering took 2821.547s.\n",
      "Model saved to file: clusters/bov_pickle_50_step_15_scale_20.sav\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Generating cluster using K-Means\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "no_clusters = 50\n",
    "kmeans = KMeans(no_clusters).fit(train_des)\n",
    "print(\"Training the K-Means clustering took %0.3fs.\" % (time.time() - t0))\n",
    "\n",
    "filename = \"clusters/bov_pickle_\"+str(no_clusters)+\"_step_\"+str(step)+\"_scale_\"+str(scale)+\".sav\"\n",
    "pickle.dump(kmeans, open(filename, 'wb'))\n",
    "print(\"Model saved to file: \" + str(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating histograms for train/test\n",
    "based on the clusters that we get K-Means\n",
    "\"\"\"\n",
    "def create_histogram(individual_data, kmeans,no_clusters):\n",
    "    hist = np.empty(shape=[0,no_clusters])\n",
    "    for i in individual_data:\n",
    "        pred_te = kmeans.predict(i)\n",
    "        h = np.histogram(pred_te,bins = np.arange(0,no_clusters+1))\n",
    "        total = i.shape[0]\n",
    "        tmp = []\n",
    "        for j in range(0,no_clusters):\n",
    "            tmp.append(h[0][j])\n",
    "        tmp = np.asarray(tmp)\n",
    "        tmp = tmp.reshape(1, no_clusters)\n",
    "        tmp = tmp/(total*1.0)\n",
    "        hist= np.append(hist,tmp,axis=0)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riddhi Rex\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:315: UserWarning: Trying to unpickle estimator KMeans from version 0.19.1 when using version 0.18.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating histograms took 1005.718s.\n"
     ]
    }
   ],
   "source": [
    "#For loading the model\n",
    "\n",
    "with open(\"descriptors_test_cluster.sav\",'rb') as fp:\n",
    "    kmeans=pickle.load(fp)\n",
    "no_clusters = 50\n",
    "\n",
    "\"\"\"\n",
    "Creating histograms or Bag of visual words\n",
    "for training and testing descriptors using K-Means cluster from before\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "train_hist = create_histogram(train_individual_des, kmeans, no_clusters)\n",
    "test_hist = create_histogram(test_individual_des, kmeans, no_clusters)\n",
    "print(\"Calculating histograms took %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_correctness(pred, test_labels, threshold=0):\n",
    "    cnt=0\n",
    "    for x, y in zip(pred, test_labels):\n",
    "        x1= np.array(x)\n",
    "        y1 = np.array(y)\n",
    "        c=0\n",
    "        for a, b in zip(x1, y1):\n",
    "            if(a!=b): c+=1\n",
    "        if c<=threshold:\n",
    "            cnt+=1\n",
    "    accuracy = cnt/len(test_labels)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    val=set(y_true).intersection(y_pred)\n",
    "    length= len(y_pred)\n",
    "    if length==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(val)/length\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    val= set(y_true).intersection(y_pred)\n",
    "    return len(val)/len(y_true)\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p =precision(y_true, y_pred)\n",
    "    r =recall(y_true, y_pred)\n",
    "    if (p+r)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2*(p*r)/(p+r)\n",
    "\n",
    "def Evaluate_accuracy(pred, true_value, model_name):\n",
    "    \n",
    "    print(\"Accuracy score for strict match is \", verify_correctness(pred, true_value, 0)*100)\n",
    "    print(\"Accuracy score for maximum of 1 mistmatch is \", verify_correctness(pred, true_value, 1)*100)\n",
    "    print(\"Accuracy score for maximum of 2 mistmatch is \", verify_correctness(pred, true_value, 2)*100)\n",
    "\n",
    "    print(\"Average Precision score \", average_precision_score(true_value, pred, average='macro'))\n",
    "\n",
    "    f_ans = 0\n",
    "    for p, tl in zip(pred, true_value):\n",
    "        f_ans+=f1(tl, p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbor model: \n",
      "Accuracy score for strict match is  36.47\n",
      "Accuracy score for maximum of 1 mistmatch is  44.32\n",
      "Accuracy score for maximum of 2 mistmatch is  54.620000000000005\n",
      "Average Precision score  0.761746369515\n",
      "F1 score: 0.22207777777774018\n",
      "Decision Tree model: \n",
      "Accuracy score for strict match is  40.53\n",
      "Accuracy score for maximum of 1 mistmatch is  47.699999999999996\n",
      "Accuracy score for maximum of 2 mistmatch is  57.78\n",
      "Average Precision score  0.779817148984\n",
      "F1 score: 0.2221111111110735\n",
      "Random Forest model: \n",
      "Accuracy score for strict match is  35.449999999999996\n",
      "Accuracy score for maximum of 1 mistmatch is  43.4\n",
      "Accuracy score for maximum of 2 mistmatch is  54.54\n",
      "Average Precision score  0.759855796946\n",
      "F1 score: 0.2220888888888513\n",
      "Gradient Boost model: \n",
      "Accuracy score for strict match is  39.410000000000004\n",
      "Accuracy score for maximum of 1 mistmatch is  46.56\n",
      "Accuracy score for maximum of 2 mistmatch is  57.67\n",
      "Average Precision score  0.793875850777\n",
      "F1 score: 0.2221444444444068\n",
      "SGD Classifier model: \n",
      "Accuracy score for strict match is  41.58\n",
      "Accuracy score for maximum of 1 mistmatch is  47.49\n",
      "Accuracy score for maximum of 2 mistmatch is  58.599999999999994\n",
      "Average Precision score  0.806925619645\n",
      "F1 score: 0.2221222222221846\n"
     ]
    }
   ],
   "source": [
    "print(\"Nearest Neighbor model: \")\n",
    "\n",
    "nnb = NearestNeighbors(n_neighbors=1, metric='euclidean')\n",
    "nnb.fit(train_hist, train_photo_labels)\n",
    "#Saving the trained model\n",
    "joblib.dump(nnb,'nnbr1.model')\n",
    "#Loading from the trained model\n",
    "nnb = joblib.load('nnbr1.model')\n",
    "dist, predictions = nnb.kneighbors(test_hist)\n",
    "pred1 = []\n",
    "\n",
    "for each in predictions:\n",
    "    pred1.append(train_photo_labels[each[0]])\n",
    "\n",
    "Evaluate_accuracy(pred1, test_photo_labels, \"nn\")\n",
    "\n",
    "print(\"Decision Tree model: \")\n",
    "\n",
    "pred2 = []\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "#Fitting the training data over the regression model\n",
    "dt.fit(train_hist, train_photo_labels)\n",
    "#Saving the trained model\n",
    "joblib.dump(dt,'dtr1.model')\n",
    "#Loading from the trained model\n",
    "dtr1 = joblib.load('dtr1.model')\n",
    "# Make predictions on the test set using the fit model.\n",
    "pred2 = dtr1.predict(test_hist)\n",
    "\n",
    "Evaluate_accuracy(pred2, test_photo_labels, \"dtc\")\n",
    "\n",
    "print(\"Random Forest model: \")\n",
    "\n",
    "pred3=[]\n",
    "rfc = RandomForestClassifier(n_estimators = 100,min_samples_split=2,max_depth=10, n_jobs=-1)\n",
    "rfc.fit(train_hist, train_photo_labels)\n",
    "#Saving the trained model\n",
    "joblib.dump(rfc,'rfc1.model')\n",
    "#Loading from the trained model\n",
    "rfc1 = joblib.load('rfc1.model')\n",
    "# Make predictions on the test set using the fit model.\n",
    "pred3 = rfc1.predict(test_hist)\n",
    "\n",
    "Evaluate_accuracy(pred3, test_photo_labels, \"rfc\")\n",
    "\n",
    "print(\"Gradient Boost model: \")\n",
    "\n",
    "pred4=[]\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "multi = MultiOutputClassifier(gbc, n_jobs=-1)\n",
    "multi.fit(train_hist, train_photo_labels)\n",
    "#Saving the trained model\n",
    "joblib.dump(multi,'gbc1.model')\n",
    "#Loading from the trained model\n",
    "gbc1 = joblib.load('gbc1.model')\n",
    "# Make predictions on the test set using the fit model.\n",
    "pred4 =gbc1.predict(test_hist)\n",
    "\n",
    "Evaluate_accuracy(pred4, test_photo_labels, \"gbc\")\n",
    "\n",
    "print(\"SGD Classifier model: \")\n",
    "\n",
    "pred5=[]\n",
    "sgd = SGDClassifier()\n",
    "multi = MultiOutputClassifier(sgd, n_jobs=-1)\n",
    "multi.fit(train_hist, train_photo_labels)\n",
    "pred5 =multi.predict(test_hist)\n",
    "\n",
    "Evaluate_accuracy(pred5, test_photo_labels, \"sgd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
