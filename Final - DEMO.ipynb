{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "import torch\n",
    "\n",
    "print(cv2.__version__)\n",
    "sift  = cv2.xfeatures2d.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names are photo_id, business_id\n",
      "Processed 234843 lines.\n",
      "234842\n",
      "Column names are business_id, labels\n",
      "Processed 2001 lines.\n",
      "2000\n",
      "Histogram distribution for the labels in the training_set\n",
      "[9163, 23679, 25603, 19453, 16829, 29454, 30772, 14825, 20300]\n"
     ]
    }
   ],
   "source": [
    "#CSV file reading\n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for photo_id ---> buisness_id\n",
    "\"\"\"\n",
    "photo_to_bus_dict = {}\n",
    "with open('train_photo_to_biz_ids.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            photo_to_bus_dict[row[0]] = row[1]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "print(len(photo_to_bus_dict))\n",
    "    \n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for business_id ---> labels\n",
    "\"\"\"\n",
    "bus_to_labels_dict = {}     \n",
    "with open('train.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            bus_to_labels_dict[row[0]] = [row[1]]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')  \n",
    "print(len(bus_to_labels_dict))\n",
    "\n",
    "\"\"\"\n",
    "Checking the distribution of labels\n",
    "in the training set\n",
    "\"\"\"\n",
    "hist_label = [0]*9\n",
    "for filename in os.listdir('training_set'):   #training_set  \n",
    "    photo_id = filename.split('.')\n",
    "    lst_labels = bus_to_labels_dict[photo_to_bus_dict[photo_id[0]]]\n",
    "    labels = lst_labels[0].split(' ')\n",
    "    if len(labels)!=0:\n",
    "        for i in labels:\n",
    "            if i is not '':\n",
    "                hist_label[int(i)]+=1\n",
    "\n",
    "print(\"Histogram distribution for the labels in the training_set\")\n",
    "print(hist_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the testing data\n",
    "Returns images and respective photo ids\n",
    "\"\"\"\n",
    "def get_test_data():\n",
    "    data = []\n",
    "    photo_ids = []\n",
    "    count=0\n",
    "    for filename in sorted(os.listdir('Bow_test/'),key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        photo_ids.append(filename.split('.')[0])\n",
    "        data.append(cv2.imread('Bow_test/'+filename,0))\n",
    "        count+=1\n",
    "    return data,photo_ids\n",
    "    \n",
    "test_data, test_photo_ids = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creates no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "def id_to_label(photo_ids):\n",
    "    photo_labels = np.empty(shape=[0,9])\n",
    "    for id in photo_ids:\n",
    "        lst_labels = bus_to_labels_dict[photo_to_bus_dict[id]]\n",
    "        ls = lst_labels[0].split(' ')\n",
    "        labels = [0]*9\n",
    "        for label in ls:\n",
    "            if label !='':\n",
    "                labels[int(label)] = 1\n",
    "        labels = np.array(labels).reshape(1,-1)\n",
    "        photo_labels = np.append(photo_labels,labels,axis=0)\n",
    "    return photo_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get the training data\n",
    "Returns images and respective photo ids\n",
    "\"\"\"\n",
    "def get_train_data():\n",
    "    data = []\n",
    "    photo_ids = []\n",
    "    count=0\n",
    "    for filename in sorted(os.listdir('training_set/'),key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        photo_ids.append(filename.split('.')[0])\n",
    "        data.append(cv2.imread('training_set/'+filename,0))\n",
    "        count+=1\n",
    "    return data,photo_ids\n",
    "    \n",
    "train_data, train_photo_ids = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1.  0.  1.  1.  1.  1.  0.]\n",
      "['1 2 4 5 6 7']\n",
      "(40000, 9)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "#Training set\n",
    "\"\"\"\n",
    "Contains no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "train_photo_labels = id_to_label(train_photo_ids)\n",
    "print(train_photo_labels[0])\n",
    "print(bus_to_labels_dict[photo_to_bus_dict[train_photo_ids[0]]])\n",
    "print(train_photo_labels.shape)\n",
    "print(train_photo_labels[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  1.]\n",
      "['6 8']\n",
      "(24, 9)\n",
      "(24,)\n"
     ]
    }
   ],
   "source": [
    "#Testing set\n",
    "\"\"\"\n",
    "Contains no_images*labels_size matrix\n",
    "with each row indicating what labels\n",
    "are assigned to each photo\n",
    "\"\"\"\n",
    "test_photo_labels = id_to_label(test_photo_ids)\n",
    "print(test_photo_labels[0])\n",
    "print(bus_to_labels_dict[photo_to_bus_dict[test_photo_ids[0]]])\n",
    "print(test_photo_labels.shape)\n",
    "print(test_photo_labels[:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of overall test images descriptors:-\n",
      "(1880, 128)\n",
      "Extracting test images descriptors took 4.418s.\n",
      "write descriptors test finished\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Function that does the extraction of dense SIFT descriptors\n",
    "params: \n",
    "   data:   vector of images       \n",
    "return:\n",
    "    temp_des: vector of descriptors for invidual image\n",
    "    des:      a combined matrix of all images\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Parameters to be considered step and scale.\n",
    "\"\"\"\n",
    "step = 50\n",
    "scale = 20\n",
    "def get_dense_descriptors(data):\n",
    "    temp_des = []\n",
    "    des = np.empty(shape=[0,128])\n",
    "    for i in data:\n",
    "        kp = [cv2.KeyPoint(y, x,scale) for y in range(0, i.shape[0], step) \n",
    "                                for x in range(0, i.shape[1], step)]\n",
    "        kp, dense_feat = sift.compute(i, kp)\n",
    "        dense_feat = np.array(dense_feat)\n",
    "        temp_des.append(dense_feat)\n",
    "        des = np.append(des,dense_feat,axis=0)\n",
    "    return temp_des, des \n",
    "\n",
    "\"\"\"\n",
    "Extracting descriptors for testing data\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "test_individual_des, test_des = get_dense_descriptors(test_data)\n",
    "print(\"Shape of overall test images descriptors:-\")\n",
    "print(test_des.shape)\n",
    "print(\"Extracting test images descriptors took %0.3fs.\" % (time.time() - t0))\n",
    "print(\"write descriptors test finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generating histograms for train/test\n",
    "based on the clusters that we get K-Means\n",
    "\"\"\"\n",
    "def create_histogram(individual_data, kmeans,no_clusters):\n",
    "    hist = np.empty(shape=[0,no_clusters])\n",
    "    for i in individual_data:\n",
    "        pred_te = kmeans.predict(i)\n",
    "        h = np.histogram(pred_te,bins = np.arange(0,no_clusters+1))\n",
    "        total = i.shape[0]\n",
    "        tmp = []\n",
    "        for j in range(0,no_clusters):\n",
    "            tmp.append(h[0][j])\n",
    "        tmp = np.asarray(tmp)\n",
    "        tmp = tmp.reshape(1, no_clusters)\n",
    "        tmp = tmp/(total*1.0)\n",
    "        hist= np.append(hist,tmp,axis=0)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating histograms took 0.030s.\n"
     ]
    }
   ],
   "source": [
    "#For loading the model\n",
    "kmeans = pickle.load(open(\"clusters/bov_pickle_new_50_step_50_scale_20.sav\",'rb'))\n",
    "no_clusters = 50\n",
    "\"\"\"\n",
    "Creating histograms or Bag of visual words\n",
    "for training and testing descriptors using K-Means cluster from before\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "test_hist = create_histogram(test_individual_des, kmeans, no_clusters)\n",
    "print(\"Calculating histograms took %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting predictions for attribute: 0\n",
      "getting predictions for attribute: 1\n",
      "getting predictions for attribute: 2\n",
      "getting predictions for attribute: 3\n",
      "getting predictions for attribute: 4\n",
      "getting predictions for attribute: 5\n",
      "getting predictions for attribute: 6\n",
      "getting predictions for attribute: 7\n",
      "getting predictions for attribute: 8\n",
      "Training took 3.336s.\n",
      "preparing output...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training 9 random classifiers sperately for each label as a target\n",
    "\"\"\"\n",
    "t0 = time.time()\n",
    "\n",
    "Ypred = []\n",
    "num_classes = 9\n",
    "clf = [None]*num_classes\n",
    "for i in range(num_classes):\n",
    "    rf = pickle.load(open(\"random_forest/classifier_\" + str(i) + \".pkl\",'rb'))\n",
    "    print(\"getting predictions for attribute:\",i)\n",
    "    y_pred = rf.predict(test_hist)\n",
    "    Ypred.append(y_pred)\n",
    "  \n",
    "print(\"Training took %0.3fs.\" % (time.time() - t0))\n",
    "print(\"preparing output...\")\n",
    "Ypred = np.vstack(Ypred)\n",
    "Ypred = np.transpose(Ypred)\n",
    "Ypred = Ypred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def verify_correctness(pred1, test_labels, threshold=0):\n",
    "    cnt=0\n",
    "    count = 0\n",
    "    write = 0\n",
    "    for x, y in zip(pred1, test_labels):\n",
    "        x1= np.array(x)\n",
    "        y1 = np.array(y)\n",
    "        c=0\n",
    "        count = count + 1\n",
    "        for a, b in zip(x1, y1):\n",
    "            if(a!=b): \n",
    "                c+=1\n",
    "        if c<=threshold:\n",
    "            cnt+=1\n",
    "    accuracy = cnt/len(test_labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for strict match is  75.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for strict match is \",verify_correctness(Ypred,test_photo_labels,0)*100)   #Strict match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator NearestNeighbors from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator SGDClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Ensemble method\n",
    "\"\"\"\n",
    "\n",
    "#Load saved models\n",
    "nnb1 = joblib.load('models/nnbr1.model')\n",
    "rfc1 = joblib.load('models/rfc1.model')\n",
    "dtr1 = joblib.load('models/dtr1.model')\n",
    "sgd1 = joblib.load('models/sgd1.model')\n",
    "\n",
    "dist, predictions = nnb1.kneighbors(test_hist)\n",
    "ensemble_pred1 = []\n",
    "for each in predictions:\n",
    "    ensemble_pred1.append(train_photo_labels[each[0]])\n",
    "ensemble_pred2 = sgd1.predict(test_hist)\n",
    "ensemble_pred3 = dtr1.predict(test_hist)\n",
    "ensemble_pred4 = rfc1.predict(test_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for strict match (Nearest Neigbhor) is  25.0\n",
      "Accuracy score for strict match (SGDClassifier) is  37.5\n",
      "Accuracy score for strict match (Decision Tree) is  16.666666666666664\n",
      "Accuracy score for strict match (Random Forest) is  29.166666666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for strict match (Nearest Neigbhor) is \",verify_correctness(ensemble_pred1,test_photo_labels,1)*100)   #Strict match\n",
    "print(\"Accuracy score for strict match (SGDClassifier) is \",verify_correctness(ensemble_pred2,test_photo_labels,1)*100)   #Strict match\n",
    "print(\"Accuracy score for strict match (Decision Tree) is \",verify_correctness(ensemble_pred3,test_photo_labels,1)*100)   #Strict match\n",
    "print(\"Accuracy score for strict match (Random Forest) is \",verify_correctness(ensemble_pred4,test_photo_labels,1)*100)   #Strict match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is active\n",
      "saving the file  3184.resnet\n",
      "saving the file  3196.resnet\n",
      "saving the file  3216.resnet\n",
      "saving the file  3220.resnet\n",
      "saving the file  3228.resnet\n",
      "saving the file  3235.resnet\n",
      "saving the file  3238.resnet\n",
      "saving the file  3239.resnet\n",
      "saving the file  3290.resnet\n",
      "37\n",
      "Column names are photo_id, business_id\n",
      "Processed 234843 lines.\n",
      "234842\n",
      "Column names are business_id, labels\n",
      "Processed 2001 lines.\n",
      "2000\n",
      "['1 4 5 6 7']\n",
      "(37,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator LinearSVC from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/akshay_mallipeddi/anaconda3/lib/python3.6/site-packages/sklearn/base.py:311: UserWarning: Trying to unpickle estimator MultiOutputClassifier from version 0.18.1 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ResNet   \n",
    "\"\"\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "#For Res-Net matrix creation\n",
    "def get_all_img_in_tensor(img, prep):\n",
    "    img_list = []\n",
    "    img1 = img[:224, :224]\n",
    "    img_list.append(prep(img1))\n",
    "    img2 = img[:224, 151:]\n",
    "    img_list.append(prep(img2))\n",
    "    img3 = img[138:362, :224]\n",
    "    img_list.append(prep(img3))\n",
    "    img4 = img[138:362, 151:]\n",
    "    img_list.append(prep(img4))\n",
    "    img5 = img[276:, :224]\n",
    "    img_list.append(prep(img5))\n",
    "    img6 = img[276:, 151:]\n",
    "    img_list.append(prep(img6))\n",
    "    img7 = img[138:362, 75:299]\n",
    "    img_list.append(prep(img7))\n",
    "    return img_list\n",
    "\n",
    "def initialize_model():\n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    input_size = 224\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is active\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model, size = initialize_model()\n",
    "model = model.to(device)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "prep = transforms.Compose([ transforms.ToTensor(), normalize ])\n",
    "\n",
    "#Matrix creation for Res-Net\n",
    "def get_img_in_tensor(img, prep):\n",
    "    img_list = []\n",
    "    resized_img = cv2.resize(img, (224, 224))\n",
    "    img_list.append(prep(resized_img))\n",
    "    img2 = resized_img\n",
    "    img_list.append(prep(img2))\n",
    "    return img_list\n",
    "\n",
    "inputpath = 'demo_testset/'\n",
    "outpath = 'demo_gendata/'\n",
    "folder_list = sorted(os.listdir(inputpath))\n",
    "try:\n",
    "    folder_list. remove('.DS_Store')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "i = 1\n",
    "out_list = np.empty(shape=[0,4096])\n",
    "for file_name in sorted(os.listdir(inputpath)):\n",
    "    img = cv2.imread(inputpath + file_name)\n",
    "    if img.shape != (500,375,3):\n",
    "        continue\n",
    "    img_list = get_all_img_in_tensor(img, prep)    ## avg of 7 images\n",
    "    inp = torch.stack(img_list)\n",
    "    inp = inp.to(device)\n",
    "    outputs = model(inp)\n",
    "    mean_output = outputs.mean(0)\n",
    "    np_out = mean_output.cpu().detach().numpy()\n",
    "    i = i + 1\n",
    "    #write the out_list to file\n",
    "    print(\"saving the file \", file_name[:-4]+'.resnet')\n",
    "    sio.savemat(outpath + file_name[:-4] +'.resnet', {'features':np_out}, do_compression=True)\n",
    "    \n",
    "\"\"\"\n",
    "Get the testing data\n",
    "Returns images and respective photo ids\n",
    "\"\"\"\n",
    "def get_test_data():\n",
    "    data = []\n",
    "    photo_ids = []\n",
    "    count=0\n",
    "    for filename in sorted(os.listdir('demo_testset/'),key=lambda x: int(os.path.splitext(x)[0])):\n",
    "        if count%5000==4999:\n",
    "            print(filename)\n",
    "        photo_ids.append(filename.split('.')[0])\n",
    "        data.append(cv2.imread('demo_testset/'+filename,0))\n",
    "        count+=1\n",
    "    return data,photo_ids\n",
    "    \n",
    "test_data, test_photo_ids = get_test_data()\n",
    "print(len(test_data))\n",
    "\n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for photo_id ---> buisness_id\n",
    "\"\"\"\n",
    "photo_to_bus_dict = {}\n",
    "with open('train_photo_to_biz_ids.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            photo_to_bus_dict[row[0]] = row[1]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')\n",
    "print(len(photo_to_bus_dict))\n",
    "    \n",
    "\"\"\"\n",
    "Generating dictionaries\n",
    "for business_id ---> labels\n",
    "\"\"\"\n",
    "bus_to_labels_dict = {}     \n",
    "with open('train.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            print(f'Column names are {\", \".join(row)}')\n",
    "            line_count += 1\n",
    "        else:\n",
    "            bus_to_labels_dict[row[0]] = [row[1]]\n",
    "            line_count += 1\n",
    "    print(f'Processed {line_count} lines.')  \n",
    "print(len(bus_to_labels_dict))\n",
    "\n",
    "def id_to_label(photo_ids):\n",
    "    photo_labels = np.empty(shape=[0,9])\n",
    "    for id in photo_ids:\n",
    "        lst_labels = bus_to_labels_dict[photo_to_bus_dict[id]]\n",
    "        ls = lst_labels[0].split(' ')\n",
    "        labels = [0]*9\n",
    "        for label in ls:\n",
    "            if label !='':\n",
    "                labels[int(label)] = 1\n",
    "        labels = np.array(labels).reshape(1,-1)\n",
    "        photo_labels = np.append(photo_labels,labels,axis=0)\n",
    "    return photo_labels\n",
    "\n",
    "test_photo_labels = id_to_label(test_photo_ids)\n",
    "print(bus_to_labels_dict[photo_to_bus_dict[test_photo_ids[0]]])\n",
    "print(test_photo_labels[:,0].shape)\n",
    "\n",
    "mat_tst=[]\n",
    "paths = glob.glob('demo_gendata/*.mat')\n",
    "for matrix in paths:\n",
    "    mat=sio.loadmat(matrix)\n",
    "    m = mat['features'].reshape(-1)\n",
    "    mat_tst.append(m)\n",
    "\n",
    "resnet1 = joblib.load('models/resnet1.model')\n",
    "resnet2 = joblib.load('models/resnet2.model')\n",
    "\n",
    "resnet1_pred = resnet1.predict(mat_tst)\n",
    "resnet2_pred = resnet2.predict(mat_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for strict match is  13.513513513513514\n",
      "Accuracy score for strict match is  10.81081081081081\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score for strict match is \",verify_correctness(resnet1_pred,test_photo_labels,2)*100)   #Strict match\n",
    "print(\"Accuracy score for strict match is \",verify_correctness(resnet2_pred,test_photo_labels,2)*100)   #Strict match"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
